# Gesture Synchronisation 

This code is for our paper titled: **GestSync: Determining who is speaking without a talking head** published at BMVC 2023 (oral).<br />
**Authors**: [Sindhu Hegde](https://sindhu-hegde.github.io), [Andrew Zisserman](https://scholar.google.com/citations?hl=en&user=UZ5wscMAAAAJ) 

|   ğŸ“ Paper   |   ğŸ“‘ Project Page    |  ğŸ›  Demo Video  |
|-----------|-------------------|---------------|
| Coming soon |    Coming soon | Coming soon | 
<br />

------
# Highlights

- The trained network can be used for 2 tasks:
  - **Synchronisation:** Predicting the temporal lags between the audio and visual streams in a video solely based on gestures.
  - **Active Speaker Detection:** Determining "who is speaking" in a multi-speaker scenario without looking at their faces.
- Works in unconstrained settings for any speaker in any language.
- Wider applications: Since the model does not require face input, it eliminates the problem of corrupted, occluded or invisible face or lip regions. 
- Complete training and inference pipelines, along with pre-trained models will be released soon. 

------
# More details coming soon...
